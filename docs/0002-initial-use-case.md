---
id: 0002-initial-use-case
title: 2 - Initial Use Case
---
Date: 2020-08-10

## Status

Proposed

## Context

We define the initial use case for `ml-aoi` spec that exposes assumptions and reasoning for specific layout choices:
providing training data source for Raster Vision model training process.

`ml-aoi` STAC Items represent a reified relation between feature rasters and ground-truth label in a machine learning training dataset.
Each `ml-aoi` Item roughly correspond to a "scene" or a training example.

### Justification for new extension

Current known STAC extensions are not suitable for this purpose. The closest match is the STAC `label` extension.
`label` extension provides a way to define either vector or raster labels over area.
However, it does not provide a mechanism to link those labels with feature images;
links with `rel` type `source` point to imagery from which labels were derived.
Sometimes this imagery will be used as feature input for model training, but not always.
The concept of source label imagery and input feature imagery are semantically distinct.
For instance it is possible to apply a single source of ground-truth building labels to train a model on either Landsat or Sentinel-2 scenes.

### Catalog Lifetime

`ml-aoi` Item links to both raster STAC item and label STAC item.
In this relationship the source raster and label items are static and long lived, being used by several `ml-aoi` catalogs.
By contrast `ml-aoi` catalog is somewhat ephemeral, it captures the training set in order to provide model reproducibility and provenance.
There can be any number of `ml-aoi` catalogs linking to the same raster and label items, while varying selection, training/testing/validation split and class configuration.

#### Training, Testing, Validation Split

During model training its important to have a consistent split between training, testing and validation data.
The selection of these sets will effect model performance and should be captured in the catalog.
Selecting these sets is possible using variety of approaches and should be delegated to the created of `ml-aoi` collection.
Couple of options are possible to represent this split in the `ml-aoi` catalog.

##### Split by Collection

Split could be generated by generating a separate collection for each set. This is a flexible approach.
However, the grouping of these collections into one cohesive training set would have to be done by convention, for instance by prefix on collection `id`.
Additionally these collections could not be easily visualized together.
Most (all?) existing STAC viewers are focused on browsing or viewing one collection at a time.

Additionally the convention of how to associate training with testing with validation set would have to be propagated into downstream tooling.
Further it would be easy to include a single item in both training and testing set without realizing it.
This is not a good choice for these reasons.

##### Split by Link property

The top-most `ml-aoi` collection has to link to each item or child catalogs.
These links could have additional property that designates the split.
This approach keeps all the items with in the same collection, which is good.

However, when ingested into STAC API this link property is often lost and is not easily queried.
Thus the split set membership would not be visible to through STAC API, which is bad.
This is not a good choice for that reason.

##### Split by Item property

Each item could have an extension specific property (ex: `ml-aoi:split`) that designates set membership.
This approach addresses the short-comings of the previous methods.

This property can be easily searched for after item is ingested into STAC API.
It is not possible to include a single item in multiple sets.
Collection can be viewed by tools that do not understand `ml-aoi` extension.


## Decision

- Test, Train, Validation split should be handled by `ml-aoi:split` Item property.

## Consequences

TBD